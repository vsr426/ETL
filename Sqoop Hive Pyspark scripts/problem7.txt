Problem 7: 
This step comprises of three substeps. Please perform tasks under each subset completely  
using sqoop pull data from MYSQL orders table into /user/cloudera/problem7/prework as AVRO data file using only one mapper
Pull the file from \user\cloudera\problem7\prework into a local folder named flume-avro
create a flume agent configuration such that it has an avro source at localhost and port number 11112,  a jdbc channel and an hdfs file sink at /user/cloudera/problem7/sink
Use the following command to run an avro client flume-ng avro-client -H localhost -p 11112 -F <<Provide your avro file path here>>
The CDH comes prepackaged with a log generating job. start_logs, stop_logs and tail_logs. Using these as an aid and provide a solution to below problem. The generated logs can be found at path /opt/gen_logs/logs/access.log  
run start_logs
write a flume configuration such that the logs generated by start_logs are dumped into HDFS at location /user/cloudera/problem7/step2. The channel should be non-durable and hence fastest in nature. The channel should be able to hold a maximum of 1000 messages and should commit after every 200 messages. 
Run the agent. 
confirm if logs are getting dumped to hdfs.  
run stop_logs.


[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username retail_dba --password cloudera --table orders --as-avrodatafile --num-mappers 1 --target-dir /user/cloudera/problem7/prework --outdir javafiles
[cloudera@quickstart ~]$ hadoop fs -get /user/cloudera/problem7/prework \user\cloudera\problem7\prework
[cloudera@quickstart ~]$ hadoop fs -get /user/cloudera/problem7/prework \user\cloudera\problem7\prework



p7.sources = avro_src
p7.channels = jdbc_chnl
p7.sinks = hdfs_snk

p7.sources.avro_src.type = avro
p7.sources.avro_src.bind = quickstart.cloudera
p7.sources.avro_src.port = 11112

p7.channels.jdbc_chnl.type = jdbc

p7.sinks.hdfs_snk.type = hdfs
p7.sinks.hdfs_snk.hdfs.path = /user/cloudera/problem7/sink
p7.sinks.hdfs_snk.hdfs.fileSuffix = .avro
p7.sinks.hdfs_snk.hdfs.fileType = DataStream
p7.sinks.hdfs_snk.hdfs.serializer = avro_event
p7.sinks.hdfs_snk.hdfs.serializer.compressionCodec = snappy

p7.sources.avro_src.channels = jdbc_chnl
p7.sinks.hdfs_snk.channel = jdbc_chnl
~

[cloudera@quickstart ~]$ flume-ng agent --name p7 --conf problem7 --conf-file p7.conf

flume-ng avro-client -H quickstart.cloudera -p 11112  -F flume-avro/part-m-00000.avro




p72.sources = cmdSrc
p72.sinks = hdfssink
p72.channels = memoryChannel

p72.sources.cmdSrc.type = exec
p72.sources.cmdSrc.command = tail -F /opt/gen_logs/logs/access.log


p72.channels.memoryChannel.type = memory
p72.channels.memoryChannel.capacity = 1000
p72.channels.memoryChannel.transactionCapacity = 200


p72.sinks.hdfssink.type = hdfs
p72.sinks.hdfssink.hdfs.path = /user/cloudera/problem7/step2
p72.sinks.hdfssink.hdfs.fileType = DataStream
p72.sinks.hdfssink.hdfs.fileSuffix = .log
p72.sinks.hdfssink.hdfs.writeFormat = Text

p72.sources.cmdSrc.channels  = memoryChannel
p72.sinks.hdfssink.channel = memoryChannel



flume-ng avro-client -H quickstart.cloudera -p 11112 -F /home/cloudera/flume-avro/part-m-00000.avro