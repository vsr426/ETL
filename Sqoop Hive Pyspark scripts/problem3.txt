Problem 3: Perform in the same sequence

Import all tables from mysql database into hdfs as avro data files. use compression and the compression codec should be snappy. data warehouse directory should be retail_stage.db
Create a metastore table that should point to the orders data imported by sqoop job above. Name the table orders_sqoop. 
Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed. select data from orders_sqoop. 
query table in impala that shows all orders belonging to a certain day. This day is when the most orders were placed. select data from order_sqoop. 
Now create a table named retail.orders_avro in hive stored as avro, the table should have same table definition as order_sqoop. Additionally, this new table should be partitioned by the order month i.e -> year-order_month.(example: 2014-01)
Load data into orders_avro table from orders_sqoop table.
Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed. select data from orders_avro
evolve the avro schema related to orders_sqoop table by adding more fields named (order_style String, order_zone Integer)
insert two more records into orders_sqoop table. 
Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed. select data from orders_sqoop
query table in impala that shows all orders belonging to a certain day. This day is when the most orders were placed. select data from orders_sqoop



 sqoop import-all-tables --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba --password cloudera --warehouse-dir retail_stage.db --compress --compression-codec "org.apache.hadoop.io.compress.SnappyCodec" --as-avrodatafile
[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/retail_stage.db
[cloudera@quickstart ~]$ hadoop fs -get /user/cloudera/retail_stage.db/orders/part-m-00000.avro
[cloudera@quickstart ~]$ avro-tools getschema part-m-00000.avro > orders-sqoop.avsc
[cloudera@quickstart ~]$ hadoop fs -put orders-sqoop.avsc /user/cloudera/schemas/order/
hive> CREATE EXTERNAL TABLE  orders_sqoop (order_id int, order_date string, order_customer_id int, order_status string) STORED AS AVRO LOCATION '/user/cloudera/retail_stage.db/orders/' TBLPROPERTIES ("avro.schema.url"="/user/cloudera/schemas/order/orders-sqoop.avsc");
hive> INSERT OVERWRITE TABLE orders_avro PARTITION(ORDER_MONTH) SELECT order_id,TO_DATE(FROM_UNIXTIME(CAST(ORDER_DATE/1000 AS BIGINT))) AS ORDER_DATE, order_customer_id,order_status,SUBSTR(TO_DATE(FROM_UNIXTIME(CAST(ORDER_DATE/1000 AS BIGINT))), 1,7) AS ORDER_MONTH FROM ORDERS_SQOOP;
